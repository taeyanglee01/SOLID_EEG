{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8dc9471d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lmdb\n",
    "import datetime\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import scipy.io\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "import h5py\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from einops import rearrange\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d96eafe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tensor(array):\n",
    "    return torch.from_numpy(array).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a341e34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    print(f'set seed {seed} is done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f66a4170",
   "metadata": {},
   "outputs": [],
   "source": [
    "LMDB = \"/pscratch/sd/a/ahhyun/EcoGFound/DATA/scaling_data_V2_Sep_2025/striped_EEG_lmdb/TUEG_1.0/1.0_TUEG/all_resample-500_highpass-0.3_lowpass-None.lmdb\"\n",
    "\n",
    "DB = lmdb.open(LMDB, readonly=True, lock=False, readahead=True, meminit=False)\n",
    "with DB.begin(write=False) as txn:\n",
    "    KEYS = pickle.loads(txn.get('__keys__'.encode()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d1b6f41f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "468470"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(KEYS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2e9fb11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== KEY VALIDATION ====\n",
      "Total keys   : 468470\n",
      "Valid keys   : 468470\n",
      "Invalid keys : 0\n",
      "\n",
      "==== SUBJECT STATS ====\n",
      "Total subjects : 5420\n",
      "\n",
      "[Top 10 subjects by #keys]\n",
      "aaaaabhz     : 13763\n",
      "aaaaahwg     : 9103\n",
      "aaaaacmq     : 8775\n",
      "aaaaaddm     : 7107\n",
      "aaaaabfu     : 5896\n",
      "aaaaahzp     : 5820\n",
      "aaaaahun     : 4914\n",
      "aaaaaghb     : 3729\n",
      "aaaaagxr     : 3321\n",
      "aaaaahyu     : 3125\n",
      "\n",
      "[Bottom 10 subjects by #keys]\n",
      "aaaaagfs     : 2\n",
      "aaaaaahy     : 1\n",
      "aaaaablr     : 1\n",
      "aaaaabmx     : 1\n",
      "aaaaabps     : 1\n",
      "aaaaabyl     : 1\n",
      "aaaaabyz     : 1\n",
      "aaaaaclx     : 1\n",
      "aaaaacqu     : 1\n",
      "aaaaaenz     : 1\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def decode_key(k):\n",
    "    if isinstance(k, (bytes, bytearray)):\n",
    "        return k.decode(\"utf-8\", errors=\"ignore\")\n",
    "    return k\n",
    "\n",
    "# 두 형태 모두 허용 + sub_id는 영어 문자열만\n",
    "KEY_PATTERN = re.compile(\n",
    "    r\"^TUEG-(?:\\d+_)?(?P<sub_id>[A-Za-z]+)_s\\d+_t\\d+_\\d+$\"\n",
    ")\n",
    "\n",
    "invalid_keys = []\n",
    "sub_ids = []\n",
    "\n",
    "for k in KEYS:\n",
    "    k_str = decode_key(k)\n",
    "    m = KEY_PATTERN.match(k_str)\n",
    "    if m is None:\n",
    "        invalid_keys.append(k_str)\n",
    "    else:\n",
    "        sub_ids.append(m.group(\"sub_id\"))\n",
    "\n",
    "# 기본 요약\n",
    "print(\"==== KEY VALIDATION ====\")\n",
    "print(f\"Total keys   : {len(KEYS)}\")\n",
    "print(f\"Valid keys   : {len(sub_ids)}\")\n",
    "print(f\"Invalid keys : {len(invalid_keys)}\")\n",
    "\n",
    "if invalid_keys:\n",
    "    print(\"\\n[Invalid key examples]\")\n",
    "    for x in invalid_keys[:10]:\n",
    "        print(x)\n",
    "\n",
    "# subject 수 계산\n",
    "unique_sub_ids = set(sub_ids)\n",
    "print(\"\\n==== SUBJECT STATS ====\")\n",
    "print(f\"Total subjects : {len(unique_sub_ids)}\")\n",
    "\n",
    "# subject별 key 개수 분포\n",
    "sub_counter = Counter(sub_ids)\n",
    "\n",
    "print(\"\\n[Top 10 subjects by #keys]\")\n",
    "for sub, cnt in sub_counter.most_common(10):\n",
    "    print(f\"{sub:12s} : {cnt}\")\n",
    "\n",
    "print(\"\\n[Bottom 10 subjects by #keys]\")\n",
    "for sub, cnt in sub_counter.most_common()[-10:]:\n",
    "    print(f\"{sub:12s} : {cnt}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3389d8c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set seed 41 is done\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from typing import List, Tuple, Union\n",
    "\n",
    "KeyT = Union[str, bytes, bytearray]\n",
    "\n",
    "random_seed(41)\n",
    "\n",
    "# 두 key 형태 모두 커버, sub_id는 \"중간 영어 문자열\"\n",
    "_KEY_RE = re.compile(\n",
    "    r\"^TUEG-(?:\\d+_)?(?P<sub_id>[A-Za-z]+)_s\\d+_t\\d+_\\d+$\"\n",
    ")\n",
    "\n",
    "def _decode_key(k: KeyT) -> str:\n",
    "    if isinstance(k, (bytes, bytearray)):\n",
    "        return k.decode(\"utf-8\", errors=\"ignore\")\n",
    "    return k\n",
    "\n",
    "def _extract_sub_id(k: KeyT) -> str:\n",
    "    s = _decode_key(k)\n",
    "    m = _KEY_RE.match(s)\n",
    "    if m is None:\n",
    "        raise ValueError(f\"Key does not match expected patterns: {s}\")\n",
    "    return m.group(\"sub_id\")\n",
    "\n",
    "\n",
    "def train_test_split_by_fold_num(\n",
    "    fold_num: int,\n",
    "    lmdb_keys: List[KeyT],\n",
    "    maxFold: int,\n",
    "    split_by_sub: bool = True,\n",
    "    seed: int = 41\n",
    ") -> Tuple[List[KeyT], List[KeyT]]:\n",
    "    \"\"\"\n",
    "    True k-fold cross-validation split.\n",
    "\n",
    "    Args:\n",
    "        fold_num: test fold index (0 <= fold_num < maxFold)\n",
    "        lmdb_keys: LMDB key list\n",
    "        maxFold: total number of folds (k)\n",
    "        split_by_sub: True → subject-wise k-fold, False → key-wise k-fold\n",
    "\n",
    "    Returns:\n",
    "        train_key_list, test_key_list\n",
    "    \"\"\"\n",
    "    if maxFold < 2:\n",
    "        raise ValueError(\"maxFold must be >= 2.\")\n",
    "    if fold_num < 0 or fold_num >= maxFold:\n",
    "        raise ValueError(f\"fold_num must be in [0, {maxFold-1}]\")\n",
    "\n",
    "    keys = list(lmdb_keys)\n",
    "\n",
    "    # 고정 seed → 모든 fold에서 assignment가 일관됨\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    if split_by_sub:\n",
    "        # -------- subject-wise k-fold --------\n",
    "        sub_to_keys = defaultdict(list)\n",
    "        invalid = []\n",
    "\n",
    "        for k in keys:\n",
    "            try:\n",
    "                sid = _extract_sub_id(k)\n",
    "                sub_to_keys[sid].append(k)\n",
    "            except ValueError:\n",
    "                invalid.append(_decode_key(k))\n",
    "\n",
    "        if invalid:\n",
    "            ex = \"\\n\".join(invalid[:10])\n",
    "            raise ValueError(\n",
    "                f\"Found {len(invalid)} invalid keys. Examples:\\n{ex}\"\n",
    "            )\n",
    "\n",
    "        subjects = np.array(list(sub_to_keys.keys()), dtype=object)\n",
    "        rng.shuffle(subjects)\n",
    "\n",
    "        # subject를 k개 fold로 분할\n",
    "        subj_folds = np.array_split(subjects, maxFold)\n",
    "        test_subjects = set(subj_folds[fold_num].tolist())\n",
    "\n",
    "        train_keys, test_keys = [], []\n",
    "        for sid, ks in sub_to_keys.items():\n",
    "            (test_keys if sid in test_subjects else train_keys).extend(ks)\n",
    "\n",
    "        return train_keys, test_keys\n",
    "\n",
    "    else:\n",
    "        # -------- key-wise k-fold --------\n",
    "        idx = np.arange(len(keys))\n",
    "        rng.shuffle(idx)\n",
    "\n",
    "        folds = np.array_split(idx, maxFold)\n",
    "        test_idx = set(folds[fold_num].tolist())\n",
    "\n",
    "        train_keys = [keys[i] for i in idx if i not in test_idx]\n",
    "        test_keys  = [keys[i] for i in idx if i in test_idx]\n",
    "\n",
    "        return train_keys, test_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "254d3cab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 374776, test: 93694\n"
     ]
    }
   ],
   "source": [
    "train_keys, test_keys = train_test_split_by_fold_num(\n",
    "    fold_num=0,\n",
    "    lmdb_keys=KEYS,\n",
    "    maxFold=5,\n",
    "    split_by_sub=False,\n",
    ")\n",
    "\n",
    "print(f\"train: {len(train_keys)}, test: {len(test_keys)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a69a812b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lmdb_get(env, key):\n",
    "    if isinstance(key, str):\n",
    "        key = key.encode(\"utf-8\")\n",
    "    with env.begin(write=False) as txn:\n",
    "        v = txn.get(key)\n",
    "    if v is None:\n",
    "        raise KeyError(f\"Key not found: {key}\")\n",
    "    return pickle.loads(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f6113df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lmdb\n",
    "\n",
    "db = lmdb.open(LMDB, readonly=True, lock=False, readahead=True, meminit=False)\n",
    "\n",
    "sample = lmdb_get(db, \"TUEG-aaaaaaai_s001_t001_10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "36587a9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17, 30, 500)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample['sample'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "43a49af6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['F3',\n",
       " 'F4',\n",
       " 'C3',\n",
       " 'C4',\n",
       " 'P3',\n",
       " 'P4',\n",
       " 'O1',\n",
       " 'O2',\n",
       " 'F7',\n",
       " 'F8',\n",
       " 'T3',\n",
       " 'T4',\n",
       " 'T5',\n",
       " 'T6',\n",
       " 'Fz',\n",
       " 'Cz',\n",
       " 'Pz']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample['data_info']['channel_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7b225f5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== KEY VALIDATION ====\n",
      "Total keys   : 91345\n",
      "Valid keys   : 91345\n",
      "Invalid keys : 0\n",
      "\n",
      "==== SUBJECT STATS ====\n",
      "Total subjects : 1084\n",
      "\n",
      "[Top 10 subjects by #keys]\n",
      "aaaaacmq     : 8775\n",
      "aaaaabfu     : 5896\n",
      "aaaaacrt     : 2828\n",
      "aaaaabbn     : 1862\n",
      "aaaaaath     : 1342\n",
      "aaaaadns     : 1279\n",
      "aaaaahzf     : 1168\n",
      "aaaaabwi     : 1022\n",
      "aaaaaacq     : 984\n",
      "aaaaaeaw     : 535\n",
      "\n",
      "[Bottom 10 subjects by #keys]\n",
      "aaaaaaan     : 3\n",
      "aaaaaait     : 3\n",
      "aaaaafqc     : 3\n",
      "aaaaadhl     : 2\n",
      "aaaaafpg     : 2\n",
      "aaaaabps     : 1\n",
      "aaaaabyz     : 1\n",
      "aaaaaclx     : 1\n",
      "aaaaacqu     : 1\n",
      "aaaaaenz     : 1\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "KEYS_TO_CHECK = test_keys\n",
    "\n",
    "def decode_key(k):\n",
    "    if isinstance(k, (bytes, bytearray)):\n",
    "        return k.decode(\"utf-8\", errors=\"ignore\")\n",
    "    return k\n",
    "\n",
    "# 두 형태 모두 허용 + sub_id는 영어 문자열만\n",
    "KEY_PATTERN = re.compile(\n",
    "    r\"^TUEG-(?:\\d+_)?(?P<sub_id>[A-Za-z]+)_s\\d+_t\\d+_\\d+$\"\n",
    ")\n",
    "\n",
    "invalid_keys = []\n",
    "sub_ids = []\n",
    "\n",
    "for k in KEYS_TO_CHECK:\n",
    "    k_str = decode_key(k)\n",
    "    m = KEY_PATTERN.match(k_str)\n",
    "    if m is None:\n",
    "        invalid_keys.append(k_str)\n",
    "    else:\n",
    "        sub_ids.append(m.group(\"sub_id\"))\n",
    "\n",
    "# 기본 요약\n",
    "print(\"==== KEY VALIDATION ====\")\n",
    "print(f\"Total keys   : {len(KEYS_TO_CHECK)}\")\n",
    "print(f\"Valid keys   : {len(sub_ids)}\")\n",
    "print(f\"Invalid keys : {len(invalid_keys)}\")\n",
    "\n",
    "if invalid_keys:\n",
    "    print(\"\\n[Invalid key examples]\")\n",
    "    for x in invalid_keys[:10]:\n",
    "        print(x)\n",
    "\n",
    "# subject 수 계산\n",
    "unique_sub_ids = set(sub_ids)\n",
    "print(\"\\n==== SUBJECT STATS ====\")\n",
    "print(f\"Total subjects : {len(unique_sub_ids)}\")\n",
    "\n",
    "# subject별 key 개수 분포\n",
    "sub_counter = Counter(sub_ids)\n",
    "\n",
    "print(\"\\n[Top 10 subjects by #keys]\")\n",
    "for sub, cnt in sub_counter.most_common(10):\n",
    "    print(f\"{sub:12s} : {cnt}\")\n",
    "\n",
    "print(\"\\n[Bottom 10 subjects by #keys]\")\n",
    "for sub, cnt in sub_counter.most_common()[-10:]:\n",
    "    print(f\"{sub:12s} : {cnt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829dc72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# /pscratch/sd/a/ahhyun/EcoGFound/DATA/scaling_data_V2_Sep_2025/striped_EEG_lmdb\n",
    "# 아현썜 pscratch의 데이터 경로 당장은 그냥 써도 되지만 추후 내 pscratch나 m4727 등으로 옮겨서 사용할 것\n",
    "\n",
    "class TUEG_for_SOLID_from_lmdb(Dataset):\n",
    "    def __init__(\n",
    "            self,\n",
    "            lmdb_dir: str,\n",
    "            maxfold: int,\n",
    "            targetfold: int,\n",
    "            seed: int,\n",
    "            train: bool,\n",
    "            split_by_sub: bool,\n",
    "    ):\n",
    "        random_seed(seed)\n",
    "        self.lmdb_dir = lmdb_dir\n",
    "        self.db = lmdb.open(lmdb_dir, readonly=True, lock=False, readahead=True, meminit=False)\n",
    "        with self.db.begin(write=False) as txn:\n",
    "            self.lmdb_keys = pickle.loads(txn.get('__keys__'.encode()))\n",
    "\n",
    "        self.train = train\n",
    "        self.split_by_sub = split_by_sub\n",
    "\n",
    "        self.maxfold = maxfold\n",
    "        self.targetfold = targetfold\n",
    "        self.data, self.target = self.make_data_and_target_by_fold(self.targetfold, self.lmdb_keys, \n",
    "                                                                   self.maxfold, self.split_by_sub)\n",
    "\n",
    "    def make_data_and_target_by_fold(self, fold, lmdb_keys, maxfold, split_by_sub):\n",
    "        self.record = []\n",
    "\n",
    "        train_data = {'input':[], 'target':[]}\n",
    "        test_data = {'input':[], 'target':[]}\n",
    "\n",
    "        # TODO : train test split by fold num\n",
    "        train_data_in_lmdb, test_data_in_lmdb = TRAIN_TEST_SPLIT_BY_FOLD_NUM(fold, lmdb_keys, maxfold, split_by_sub)\n",
    "\n",
    "\n",
    "        if self.train:\n",
    "            for train_data_idx in train_data_in_lmdb:\n",
    "\n",
    "                # TODO : get proper seg_in and seg_out by input idx\n",
    "                seg_in, seg_out = self.segmentation_from_idx(train_data_idx)\n",
    "\n",
    "                train_data['input'] += seg_in\n",
    "                train_data['target'] += seg_out\n",
    "\n",
    "                data = train_data['input']\n",
    "                target = train_data['target']\n",
    "        \n",
    "        else:\n",
    "            for test_data_idx in test_data_in_lmdb:\n",
    "\n",
    "                seg_in, seg_out = self.segmentation_from_idx(test_data_idx)\n",
    "\n",
    "                test_data['input'] += seg_in\n",
    "                test_data['target'] += seg_out\n",
    "\n",
    "                data = test_data['input']\n",
    "                target = test_data['target']\n",
    "\n",
    "\n",
    "        return data, target\n",
    "\n",
    "    def segmentation_from_idx(idx):\n",
    "        seg_in = None\n",
    "        seg_out = None\n",
    "        return seg_in, seg_out\n",
    "\n",
    "    def train_test_split_by_fold_num(fold_num, lmdb_keys, train_ratio, split_by_sub):\n",
    "        train_key_list = None\n",
    "        test_key_list = None\n",
    "        return train_key_list, test_key_list\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        input_, target_ = self.data[idx], self.target[idx]\n",
    "        # TODO : 이거 meta 줄 떄 time은 time이고 spatial을 아예 grid에 맞게 주는게 좋을 듯 / Grid 여기서 받게 하자\n",
    "        i = None\n",
    "        o = None\n",
    "        im = None\n",
    "        om = None\n",
    "        return i, o, im, om"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe198315",
   "metadata": {},
   "outputs": [],
   "source": [
    "TORCHEEG_2DGRID = [\n",
    "    ['-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-'],\n",
    "    ['-', '-', '-', '-', 'FP1', 'FPZ', 'FP2', '-', '-', '-', '-'],\n",
    "    ['-', '-', 'AF7', '-', 'AF3', 'AFZ', 'AF4', '-', 'AF8', '-', '-'],\n",
    "    ['F9', 'F7', 'F5', 'F3', 'F1', 'FZ', 'F2', 'F4', 'F6', 'F8', 'F10'],\n",
    "    ['FT9', 'FT7', 'FC5', 'FC3', 'FC1', 'FCZ', 'FC2', 'FC4', 'FC6', 'FT8', 'FT10'], \n",
    "    ['T9', 'T7', 'C5', 'C3', 'C1', 'CZ', 'C2', 'C4', 'C6', 'T8', 'T10'],\n",
    "    ['TP9', 'TP7', 'CP5', 'CP3', 'CP1', 'CPZ', 'CP2', 'CP4', 'CP6', 'TP8', 'TP10'], \n",
    "    ['P9', 'P7', 'P5', 'P3', 'P1', 'PZ', 'P2', 'P4', 'P6', 'P8', 'P10'],\n",
    "    ['-', '-', 'PO7', 'PO5', 'PO3', 'POZ', 'PO4', 'PO6', 'PO8', '-', '-'],\n",
    "    ['-', '-', '-', 'CB1', 'O1', 'OZ', 'O2', 'CB2', '-', '-', '-'],\n",
    "    ['-', '-', '-', '-', '-', 'IZ', '-', '-', '-', '-', '-']\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fb5d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEGToGrid(Dataset):\n",
    "    def __init__(self, base_dataset,):\n",
    "        self.base_dataset = base_dataset\n",
    "        self.mean = float(self.base_dataset.mean)\n",
    "        self.std = float(self.base_dataset.std)\n",
    "\n",
    "    def TorchEEG_Grid(self, channel_list, grid_templete=TORCHEEG_2DGRID, H=11, W=11):\n",
    "        \"\"\"\n",
    "        2D Grid based on TorchEEG 2D Grid\n",
    "        input 10-10 coord channel name index \n",
    "        output is grid of channel input\n",
    "        \"\"\"\n",
    "        grid = torch.zeros(H, W, dtype=torch.float32)\n",
    "        mask = torch.zeros(H, W, dtype=torch.float32)\n",
    "        return grid, mask\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.base_dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        i,o,im,om = self.base[idx]\n",
    "\n",
    "        target_grid = None\n",
    "        target_mask = None\n",
    "        cond = None\n",
    "\n",
    "        return target_grid, target_mask, cond, self.mean, self.std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200d6917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set some future argparse\n",
    "\n",
    "# TUEG_1.0 path in lucy's pscratch\n",
    "# /pscratch/sd/a/ahhyun/EcoGFound/DATA/scaling_data_V2_Sep_2025/striped_EEG_lmdb/TUEG_1.0/1.0_TUEG/all_resample-500_highpass-0.3_lowpass-None.lmdb\n",
    "LMDB_DIR = \"/pscratch/sd/a/ahhyun/EcoGFound/DATA/scaling_data_V2_Sep_2025/striped_EEG_lmdb/TUEG_1.0/1.0_TUEG/all_resample-500_highpass-0.3_lowpass-None.lmdb\"\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c798205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Train and Test dataset and dataloader\n",
    "\n",
    "train_eeg = TUEG_for_SOLID_from_lmdb(mdb_dir=LMDB_DIR,\n",
    "                         maxFolds=5,\n",
    "                         seed=41,\n",
    "                         train=True,)\n",
    "test_eeg = TUEG_for_SOLID_from_lmdb(lmdb_dir=LMDB_DIR,\n",
    "                         maxFolds=5,\n",
    "                         seed=41,\n",
    "                         train=False,\n",
    "                         )\n",
    "\n",
    "train_set = EEGToGrid(train_eeg)\n",
    "test_set = EEGToGrid(test_eeg)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True, drop_last=True, num_worker=2, pin_memory=True)\n",
    "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False, drop_last=False, num_worker=2, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "853602c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEGDelhiLike(Dataset):\n",
    "    \"\"\"\n",
    "    LMDB에서 EEG raw를 읽어 (i, o, im, om)을 반환.\n",
    "    - i:  (ctx_len*C_used, 1)   : 과거 ctx_len step의 '신호'\n",
    "    - o:  (C_used, 1)           : 다음 step의 '신호'\n",
    "    - im: (ctx_len*C_used, 3)   : (x, y, t_id)  (x,y는 0~1, t_id는 ctx_len개의 유니크 값)\n",
    "    - om: (C_used, 3)           : (x, y, t_id=0) (t_id는 크게 중요하지 않으면 0으로 둬도 됨)\n",
    "\n",
    "    전제:\n",
    "    - sample은 (C, N) 또는 (C, T, Fs) 등인데, 마지막 축들을 펼쳐서 (C, N)으로 사용.\n",
    "    - channel 좌표는 channel_pos에 dict 형태로 제공: {ch_idx: (x01, y01)} (0~1 정규화)\n",
    "      -> 좌표 없는 채널은 제외하거나(기본), 혹은 (0,0)으로 처리 가능.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_dir: str,\n",
    "        keys: list[str],\n",
    "        channel_pos: dict,             # {ch: (x01,y01)} 0~1\n",
    "        ctx_len: int = 9,\n",
    "        horizon: int = 1,              # 다음 몇 step을 타깃으로 할지 (여기선 1만 쓰는 게 wrapper와 잘 맞음)\n",
    "        step: int = 50,                # \"1 step\"이 몇 sample인지 (예: 500Hz에서 50이면 100ms)\n",
    "        start_offset: int = 0,          # 윈도우 시작 오프셋\n",
    "        pick_channels: list[int] | None = None,  # 특정 채널만 쓰고 싶으면\n",
    "        normalize: str = \"z_trial\",     # \"none\" | \"z_trial\" | \"z_ch_trial\"\n",
    "        transform=None,\n",
    "        return_info: bool = False,\n",
    "        amp_scale: float = 1.0          # 기존 코드의 /100 같은 스케일링\n",
    "    ):\n",
    "        self.data_dir = data_dir\n",
    "        self.keys = keys\n",
    "        self.channel_pos = channel_pos\n",
    "        self.ctx_len = ctx_len\n",
    "        self.horizon = horizon\n",
    "        self.step = step\n",
    "        self.start_offset = start_offset\n",
    "        self.pick_channels = pick_channels\n",
    "        self.normalize = normalize\n",
    "        self.transform = transform\n",
    "        self.return_info = return_info\n",
    "        self.amp_scale = amp_scale\n",
    "\n",
    "        # LMDB 핸들\n",
    "        self.db = lmdb.open(\n",
    "            self.data_dir, readonly=True, lock=False,\n",
    "            readahead=True, meminit=False\n",
    "        )\n",
    "\n",
    "        # 사용할 채널 목록 확정 (좌표가 있는 채널만 기본 사용)\n",
    "        if self.pick_channels is None:\n",
    "            self.used_channels = sorted([ch for ch in self.channel_pos.keys()])\n",
    "        else:\n",
    "            self.used_channels = [ch for ch in self.pick_channels if ch in self.channel_pos]\n",
    "\n",
    "        if len(self.used_channels) == 0:\n",
    "            raise ValueError(\"used_channels가 비었습니다. channel_pos / pick_channels를 확인하세요.\")\n",
    "\n",
    "        # (C_used, 2) 좌표 텐서 캐시\n",
    "        xy = [self.channel_pos[ch] for ch in self.used_channels]\n",
    "        self.xy = torch.tensor(xy, dtype=torch.float32)  # (C_used,2)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.keys)\n",
    "\n",
    "    def _flatten_to_CN(self, data: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        data를 (C, N)으로 변환.\n",
    "        예:\n",
    "          (C, N) -> 그대로\n",
    "          (C, T, Fs) -> (C, T*Fs)\n",
    "          (C, ..., ...) -> (C, prod(rest))\n",
    "        \"\"\"\n",
    "        if data.ndim < 2:\n",
    "            raise ValueError(f\"EEG sample ndim이 {data.ndim}입니다. 최소 (C,N) 형태가 필요합니다.\")\n",
    "        C = data.shape[0]\n",
    "        return data.reshape(C, -1)\n",
    "\n",
    "    def _apply_normalize(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: (C_used, Nwin)\n",
    "        \"\"\"\n",
    "        if self.normalize == \"none\":\n",
    "            return x\n",
    "        if self.normalize == \"z_trial\":\n",
    "            mean = x.mean()\n",
    "            std = x.std().clamp_min(1e-6)\n",
    "            return (x - mean) / std\n",
    "        if self.normalize == \"z_ch_trial\":\n",
    "            mean = x.mean(dim=1, keepdim=True)\n",
    "            std = x.std(dim=1, keepdim=True).clamp_min(1e-6)\n",
    "            return (x - mean) / std\n",
    "        raise ValueError(f\"normalize='{self.normalize}'는 지원하지 않습니다.\")\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        key = self.keys[idx]\n",
    "\n",
    "        with self.db.begin(write=False) as txn:\n",
    "            pair = pickle.loads(txn.get(key.encode()))\n",
    "\n",
    "        data = pair[\"sample\"]          # (C, ...) raw\n",
    "        label = pair.get(\"label\", None)\n",
    "        data_info = pair.get(\"data_info\", {})\n",
    "\n",
    "        data = to_tensor(data).float() / float(self.amp_scale)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            data = self.transform(data)\n",
    "\n",
    "        # (C, N)로 펼치기\n",
    "        data = self._flatten_to_CN(data)\n",
    "\n",
    "        # 채널 선택 + 좌표 없는 채널 제거\n",
    "        data = data[self.used_channels, :]   # (C_used, N)\n",
    "\n",
    "        # 한 샘플(trial) 내에서 \"ctx_len + horizon\" 만큼의 step을 만들기 위해 필요한 길이\n",
    "        need = (self.ctx_len + self.horizon) * self.step\n",
    "        if data.shape[1] < self.start_offset + need:\n",
    "            # 너무 짧으면 뒤에서 잘리지 않게 start를 앞으로 당김 (최소 동작)\n",
    "            start = max(0, data.shape[1] - need)\n",
    "        else:\n",
    "            start = self.start_offset\n",
    "\n",
    "        # (C_used, need_samples) 구간 추출\n",
    "        seg = data[:, start:start + need]     # (C_used, need)\n",
    "\n",
    "        # 정규화 (trial 기준 / 채널별)\n",
    "        seg = self._apply_normalize(seg)\n",
    "\n",
    "        # step 단위로 reshape: (C_used, ctx_len+horizon, step)\n",
    "        seg = seg.reshape(seg.shape[0], self.ctx_len + self.horizon, self.step)\n",
    "\n",
    "        # 여기서 \"각 step에서 grid로 뿌릴 스칼라\"를 선택해야 함.\n",
    "        # raw를 최대한 유지하려면 step 내의 대표값을 하나 뽑아야 하는데,\n",
    "        # 가장 단순히 마지막 샘플(또는 평균)을 사용.\n",
    "        # - 마지막샘플: seg[..., -1]\n",
    "        # - 평균: seg.mean(-1)\n",
    "        per_step_val = seg[..., -1]  # (C_used, ctx_len+horizon)\n",
    "\n",
    "        # input/target 분리\n",
    "        x_in = per_step_val[:, :self.ctx_len]                 # (C_used, ctx_len)\n",
    "        x_out = per_step_val[:, self.ctx_len:self.ctx_len+1]  # (C_used, 1)\n",
    "\n",
    "        # (ctx_len*C_used, 1)로 펼치기\n",
    "        # time id는 -8..0 같이 ctx_len개의 유니크 값이 되도록 구성 (wrapper의 unique==9 요구 대응)\n",
    "        t_ids = torch.arange(-(self.ctx_len - 1), 1, dtype=torch.float32)  # (ctx_len,)\n",
    "        # 각 time에 대해 채널 C_used개씩 반복되도록 구성\n",
    "        # im: (ctx_len*C_used, 3) = [x, y, t_id]\n",
    "        C_used = x_in.shape[0]\n",
    "\n",
    "        # i: (ctx_len*C_used, 1)\n",
    "        i = x_in.T.contiguous().reshape(self.ctx_len * C_used, 1)  # time-major -> flatten\n",
    "\n",
    "        # o: (C_used, 1)\n",
    "        o = x_out.reshape(C_used, 1)\n",
    "\n",
    "        # im 만들기: 시간별로 좌표를 반복\n",
    "        # time 0: all channels, time 1: all channels ... 형태\n",
    "        xy_rep = self.xy.unsqueeze(0).repeat(self.ctx_len, 1, 1)          # (ctx_len, C, 2)\n",
    "        t_rep  = t_ids.view(self.ctx_len, 1, 1).repeat(1, C_used, 1)      # (ctx_len, C, 1)\n",
    "        im = torch.cat([xy_rep, t_rep], dim=-1).reshape(self.ctx_len * C_used, 3)\n",
    "\n",
    "        # om: (C_used, 3) = [x, y, 0]\n",
    "        om = torch.cat([self.xy, torch.zeros(C_used, 1)], dim=-1)\n",
    "\n",
    "        if self.return_info:\n",
    "            return i, o, im, om, label, data_info\n",
    "        return i, o, im, om"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2c0642",
   "metadata": {},
   "outputs": [],
   "source": [
    "LMDB_DIR = \"/pscratch/sd/t/tylee/Dataset/1109_Physio_500Hz\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de179f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEG_from_lmdb(Dataset):\n",
    "    def __init__(self, data_dir, transform, return_info):\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "        self.return_info = return_info\n",
    "\n",
    "    def lmdb_to_data(self, idx):\n",
    "        self.db = lmdb.open(self.data_dir, readonly=True, lock=False, readahead=True, meminit=False)\n",
    "        key = self.keys[idx]\n",
    "        with self.db.begin(write=False) as txn:\n",
    "            pair = pickle.loads(txn.get(key.encode()))\n",
    "        data = pair['sample']\n",
    "        label = pair['label']\n",
    "        data_info = pair.get('data_info', {})\n",
    "        \n",
    "        data = to_tensor(data)\n",
    "        if self.transform is not None:\n",
    "            data = self.transform(data)\n",
    "        if self.return_info:\n",
    "            return data/100, label, data_info\n",
    "        else:\n",
    "            return data/100, label\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_, target_ = self.data[idx], self.target[idx]\n",
    "        in_ = torch.from_numpy(input_.astype(np.float32))\n",
    "        out_ = torch.from_numpy(target_.astype(np.float32))\n",
    "        # print(in_)\n",
    "        \n",
    "        # Normalize pm2.5 values\n",
    "        in_[..., 0] = self.normalize_z(in_[..., 0])\n",
    "        out_[..., 0] = self.normalize_z(out_[..., 0])\n",
    "        \n",
    "        in_[..., 1] = in_[..., 1] / 1440\n",
    "        out_[..., 1] = out_[..., 1] / 1440\n",
    "        \n",
    "        timegap = out_[..., 1:2][0] # get the gap between t+1 and t (ignoring t-1, and t-2)\n",
    "        \n",
    "        in_ = torch.cat([in_[..., 0:1], in_[..., 2:], in_[..., 1:2], ], dim=-1)\n",
    "        out_ = torch.cat([out_[..., 0:1], out_[..., 2:], out_[..., 1:2], ], dim=-1)\n",
    "        \n",
    "        in_[..., 1] = self.normalize(in_[..., 1], self.latmin, self.latmax)\n",
    "        out_[..., 1] = self.normalize(out_[..., 1], self.latmin, self.latmax)\n",
    "        in_[..., 2] = self.normalize(in_[..., 2], self.longmin, self.longmax)\n",
    "        out_[..., 2] = self.normalize(out_[..., 2], self.longmin, self.longmax)\n",
    "        \n",
    "        i = in_[..., 0:1]\n",
    "        im = in_[..., 1:]\n",
    "        o = out_[..., 0:1]\n",
    "        om = out_[..., 1:]\n",
    "        \n",
    "        return i, o, im, om"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c5fd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing torch.dataset\n",
    "\n",
    "def xform_day(day):\n",
    "    arr = [0, 30, 61]\n",
    "    w = 0 if day <= 30 else 1 if day <= 61 else 2\n",
    "    mon = ['2020-11-', '2020-12-', '2021-01-'][w]\n",
    "    date = mon + '{:02d}'.format(day - arr[w])\n",
    "    return date\n",
    "\n",
    "\n",
    "def get_suffixes(mode):\n",
    "    suffixes = []\n",
    "    if 'C' in mode or 'A' in mode:\n",
    "        suffixes.append('train')\n",
    "    if 'D' in mode or 'B' in mode:\n",
    "        suffixes.append('test')\n",
    "    return suffixes\n",
    "\n",
    "def rename_cols(data):\n",
    "    data.rename(\n",
    "        columns={'dateTime': 'time', 'lat': 'latitude', 'long': 'longitude', 'pm2_5': 'PM25_Concentration',\n",
    "                 'pm10': 'PM10_Concentration'}, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "def torch1dgrid(num, bot=0, top=1):\n",
    "    arr = torch.linspace(bot, top, steps=num)\n",
    "    mesh = torch.stack([arr], dim=1)\n",
    "    return mesh.squeeze(-1)\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from einops import rearrange        \n",
    "class Delhi(Dataset):\n",
    "    def __init__(\n",
    "        self, mode_t, mode_p, canada, train_days, \n",
    "        maxFolds = 5, target_fold = 0, temporal_scaling=1, spatiotemporal=1, data_dir='/pscratch/sd/d/dpark1/AirDelhi/delhi/processed', \n",
    "        seed=10, nTrainStartDay = 15, nTestStartDay = 75, nTotalDays = 91, train=True):\n",
    "        \n",
    "        self.mode_t = mode_t\n",
    "        self.mode_p = mode_p\n",
    "        self.train_days = train_days\n",
    "        self.train = train\n",
    "        self.maxFolds = maxFolds\n",
    "        self.target_fold = target_fold\n",
    "        self.temporal_scaling = temporal_scaling\n",
    "        self.spatiotemporal = spatiotemporal\n",
    "        self.data_dir = data_dir\n",
    "        self.nTestStartDay = nTestStartDay\n",
    "        self.nTrainStartDay = nTrainStartDay\n",
    "        self.nTotalDays = nTotalDays\n",
    "        \n",
    "        np.random.seed(seed)        \n",
    "        \n",
    "        self.train_suffix = get_suffixes(mode_t)\n",
    "                \n",
    "        if spatiotemporal < 0 and mode_t == 'AB' and mode_p == 'CD':\n",
    "            # Forecasting, single fold is enough\n",
    "            maxFolds = 1\n",
    "    \n",
    "        self.folds = [i for i in range(maxFolds)]\n",
    "        \n",
    "        self.data, self.target = self.proc_custom(target_fold)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def get_normalize_params(self, target):\n",
    "        all_signal = []\n",
    "        for a in target:\n",
    "            all_signal += list(a[..., 0])\n",
    "        self.mean, self.std = np.array(all_signal).mean(), np.array(all_signal).std()\n",
    "    \n",
    "    def get_spatial_norm_parameters(self, arr_of_days):\n",
    "        \"\"\"\"minmax normalization\"\"\"\n",
    "        latmin = 10e10\n",
    "        latmax = -10e10\n",
    "        longmin = 10e10\n",
    "        longmax = -10e10\n",
    "        for arr in arr_of_days:\n",
    "            minned = arr.min(0)\n",
    "            # print(minned[0])\n",
    "            if minned[2] < latmin:\n",
    "                latmin = minned[2]\n",
    "            if minned[3] < longmin:\n",
    "                longmin = minned[3]\n",
    "                \n",
    "            maxed = arr.max(0)\n",
    "            # print(maxed)\n",
    "            if maxed[2] > latmax:\n",
    "                latmax = maxed[2]\n",
    "            if maxed[3] > longmax:\n",
    "                longmax = maxed[3]\n",
    "        self.latmin, self.latmax, self.longmin, self.longmax =latmin, latmax, longmin, longmax\n",
    "    \n",
    "    def make_data_by_time(self, arr_of_days, t_in = 9, reverse=False, day = 0):\n",
    "        seg_by_time = []\n",
    "        uniq_times = np.unique(arr_of_days[..., 1])\n",
    "        \n",
    "        for t in uniq_times:\n",
    "            idx_ = arr_of_days[..., 1] == t\n",
    "            seg_by_time.append(arr_of_days[idx_])\n",
    "        \n",
    "        in_ = []\n",
    "        out_ = []\n",
    "        for i in range(len(seg_by_time) - t_in):\n",
    "            temp_in = []\n",
    "            for t_ in range(t_in):\n",
    "                temp_in.append(seg_by_time[i + t_])\n",
    "            \n",
    "            # normalize time to relative scale by the last one of the encoder\n",
    "            in_cand = np.copy(np.concatenate(temp_in, axis=0))\n",
    "            out_cand = np.copy(seg_by_time[i + t_in])\n",
    "            \n",
    "            last_enc_t = in_cand[..., 1][-1]\n",
    "            in_cand[..., 1] -= last_enc_t\n",
    "            out_cand[..., 1] -= last_enc_t\n",
    "            in_.append(in_cand)\n",
    "            out_.append(out_cand)\n",
    "            self.day_record.append(day)\n",
    "            \n",
    "            # reverse it\n",
    "            if reverse:\n",
    "                out_cand = np.copy(seg_by_time[i])\n",
    "                temp_in = temp_in[1:]\n",
    "                temp_in.append(seg_by_time[i + t_in])\n",
    "                in_cand = np.copy(np.concatenate(temp_in, axis=0))\n",
    "                last_enc_t = in_cand[..., 1][0]\n",
    "                in_cand[..., 1] -= last_enc_t\n",
    "                out_cand[..., 1] -= last_enc_t\n",
    "                \n",
    "                in_.append(in_cand)\n",
    "                out_.append(out_cand)\n",
    "                self.day_record.append(day)\n",
    "        \n",
    "        \n",
    "        return in_, out_\n",
    "    \n",
    "    def proc_custom(self, fold):\n",
    "        \n",
    "        self.day_record = []\n",
    "        \n",
    "        train_data = {'input':[], 'target':[]}\n",
    "        test_data = {'input':[], 'target':[]}\n",
    "        \n",
    "        for day in range(self.nTrainStartDay, self.nTestStartDay):\n",
    "            date = []\n",
    "            for i in range(self.train_days,-1,-1):\n",
    "                date.append(xform_day(day-i))\n",
    "\n",
    "            train_input,train_output,test_input,test_output = self.process_np(fold, date)\n",
    "            train_in = np.concatenate([train_output[..., np.newaxis], train_input], axis=1) # 1 days\n",
    "            train_out = np.concatenate([test_output[..., np.newaxis], test_input], axis=1) # 1 day\n",
    "            \n",
    "            seg_in, seg_out = self.make_data_by_time(train_in, day = day)\n",
    "            \n",
    "            train_data['input'] += seg_in\n",
    "            train_data['target'] += seg_out            \n",
    "        \n",
    "        \n",
    "        \n",
    "        seg_in, seg_out = self.make_data_by_time(train_out)\n",
    "        train_data['input'] += seg_in\n",
    "        train_data['target'] += seg_out\n",
    "            \n",
    "        \n",
    "        for day in range(self.nTestStartDay, self.nTotalDays+1):\n",
    "            date = []\n",
    "            for i in range(self.train_days,-1,-1):\n",
    "                date.append(xform_day(day-i))\n",
    "\n",
    "            train_input,train_output,test_input,test_output = self.process_np(fold, date)\n",
    "            test_in = np.concatenate([train_output[..., np.newaxis], train_input], axis=1) # 1 days\n",
    "            test_out = np.concatenate([test_output[..., np.newaxis], test_input], axis=1) # 1 day\n",
    "\n",
    "            seg_in, seg_out = self.make_data_by_time(test_in, reverse = False)\n",
    "            \n",
    "            test_data['input'] += seg_in\n",
    "            test_data['target'] += seg_out            \n",
    "            \n",
    "        seg_in, seg_out = self.make_data_by_time(test_out, reverse = False)\n",
    "        test_data['input'] += seg_in\n",
    "        test_data['target'] += seg_out\n",
    "\n",
    "        self.get_normalize_params(train_data['target']) \n",
    "        self.get_spatial_norm_parameters(train_data['target'])\n",
    "            \n",
    "        if self.train:\n",
    "            data = train_data['input']\n",
    "            target = train_data['target']\n",
    "            print(len(data), len(target))\n",
    "            \n",
    "        else:\n",
    "            data = test_data['input']\n",
    "            target = test_data['target']\n",
    "            print(len(data), len(target))\n",
    "\n",
    "        return data, target        \n",
    "    \n",
    "    \n",
    "\n",
    "    def process_np(self, fold, date):\n",
    "        tmStart = datetime.datetime.now()\n",
    "        train_input,train_output,test_input,test_output = self.return_data_time(fold=fold, data=date, with_scaling=True)\n",
    "        return train_input,train_output,test_input,test_output\n",
    "    \n",
    "    def return_data_time(self, fold, data, with_scaling):\n",
    "        train_input = None\n",
    "        if 'A' in self.mode_t or 'B' in self.mode_t:\n",
    "            for idx,dt in enumerate(data[:-1]):\n",
    "                for suffix in self.train_suffix:\n",
    "                    input = pd.read_csv(self.data_dir+'/'+dt+'_f'+str(fold)+'_'+suffix+'.csv')\n",
    "                    # if self.temporal_scaling:\n",
    "                    #     input.dateTime += idx * 24 * 60\n",
    "                    train_input = pd.concat((train_input, input))\n",
    "                    \n",
    "        if 'C' in self.mode_t:\n",
    "            input = pd.read_csv(self.data_dir + '/' + data[-1] + '_f' + str(fold) + '_train.csv')\n",
    "            # if self.temporal_scaling:\n",
    "            #     input.dateTime += (len(data)-1) * 24 * 60\n",
    "            train_input = pd.concat((train_input, input))\n",
    "\n",
    "        test_input = pd.read_csv(self.data_dir+'/'+data[-1]+'_f'+str(fold)+'_test.csv')\n",
    "        \n",
    "        if 'C' in self.mode_p:\n",
    "            input = pd.read_csv(self.data_dir + '/' + data[-1] + '_f' + str(fold) + '_train.csv')\n",
    "            test_input = pd.concat((input, test_input))\n",
    "            \n",
    "        # if self.temporal_scaling:\n",
    "        #     test_input.dateTime += (len(data)-1) * 24 * 60\n",
    "\n",
    "        return self.return_data_0(train_input, test_input, with_scaling)\n",
    "\n",
    "    \n",
    "    def return_data_0(self, train_input, test_input, with_scaling):\n",
    "        train_output = np.array(train_input['pm2_5'])\n",
    "        train_input = train_input[['dateTime','lat','long']]\n",
    "        test_output = np.array(test_input['pm2_5'])\n",
    "        test_input = test_input[['dateTime','lat','long']]\n",
    "\n",
    "        # if with_scaling:\n",
    "        #     scaler = MinMaxScaler().fit(train_input)\n",
    "        #     if self.temporal_scaling:\n",
    "        #         data = scaler.transform(pd.concat((train_input, test_input)))\n",
    "        #         test_input = data[len(train_input):]\n",
    "        #         train_input = data[:len(train_input)]\n",
    "        #     else:\n",
    "        #         train_input = scaler.transform(train_input)\n",
    "        #         test_input = scaler.transform(test_input)\n",
    "        return train_input,train_output,test_input,test_output\n",
    "\n",
    "    def set_target_fold(self, fold=0):\n",
    "        self.fold = fold\n",
    "        print('target fold set to {}'.format(self.fold))\n",
    "        \n",
    "    def normalize_z(self, arr):\n",
    "        return (arr - self.mean) / self.std\n",
    "    \n",
    "    def normalize(self, data, min_, max_):\n",
    "        return (data - min_) / (max_ - min_)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_, target_ = self.data[idx], self.target[idx]\n",
    "        in_ = torch.from_numpy(input_.astype(np.float32))\n",
    "        out_ = torch.from_numpy(target_.astype(np.float32))\n",
    "        # print(in_)\n",
    "        \n",
    "        # Normalize pm2.5 values\n",
    "        in_[..., 0] = self.normalize_z(in_[..., 0])\n",
    "        out_[..., 0] = self.normalize_z(out_[..., 0])\n",
    "        \n",
    "        in_[..., 1] = in_[..., 1] / 1440\n",
    "        out_[..., 1] = out_[..., 1] / 1440\n",
    "        \n",
    "        timegap = out_[..., 1:2][0] # get the gap between t+1 and t (ignoring t-1, and t-2)\n",
    "        \n",
    "        in_ = torch.cat([in_[..., 0:1], in_[..., 2:], in_[..., 1:2], ], dim=-1)\n",
    "        out_ = torch.cat([out_[..., 0:1], out_[..., 2:], out_[..., 1:2], ], dim=-1)\n",
    "        \n",
    "        in_[..., 1] = self.normalize(in_[..., 1], self.latmin, self.latmax)\n",
    "        out_[..., 1] = self.normalize(out_[..., 1], self.latmin, self.latmax)\n",
    "        in_[..., 2] = self.normalize(in_[..., 2], self.longmin, self.longmax)\n",
    "        out_[..., 2] = self.normalize(out_[..., 2], self.longmin, self.longmax)\n",
    "        \n",
    "        i = in_[..., 0:1]\n",
    "        im = in_[..., 1:]\n",
    "        o = out_[..., 0:1]\n",
    "        om = out_[..., 1:]\n",
    "        \n",
    "        return i, o, im, om"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fingerflex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
