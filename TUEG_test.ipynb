{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8dc9471d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lmdb\n",
    "import datetime\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from typing import List, Tuple, Union\n",
    "\n",
    "import scipy.io\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "import h5py\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from einops import rearrange\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d96eafe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tensor(array):\n",
    "    return torch.from_numpy(array).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a341e34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    print(f'set seed {seed} is done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3389d8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "KeyT = Union[str, bytes, bytearray]\n",
    "\n",
    "# two type of TUEG sub id pattern handle both\n",
    "_KEY_RE = re.compile(\n",
    "    r\"^TUEG-(?:\\d+_)?(?P<sub_id>[A-Za-z]+)_s\\d+_t\\d+_\\d+$\"\n",
    ")\n",
    "\n",
    "def _decode_key(k: KeyT) -> str:\n",
    "    if isinstance(k, (bytes, bytearray)):\n",
    "        return k.decode(\"utf-8\", errors=\"ignore\")\n",
    "    return k\n",
    "\n",
    "def _extract_sub_id(k: KeyT) -> str:\n",
    "    s = _decode_key(k)\n",
    "    m = _KEY_RE.match(s)\n",
    "    if m is None:\n",
    "        raise ValueError(f\"Key does not match expected patterns: {s}\")\n",
    "    return m.group(\"sub_id\")\n",
    "\n",
    "\n",
    "def train_test_split_by_fold_num(\n",
    "    fold_num: int,\n",
    "    lmdb_keys: List[KeyT],\n",
    "    maxFold: int,\n",
    "    split_by_sub: bool = True,\n",
    "    seed: int = 41\n",
    ") -> Tuple[List[KeyT], List[KeyT]]:\n",
    "    \"\"\"\n",
    "    True k-fold cross-validation split.\n",
    "\n",
    "    Args:\n",
    "        fold_num: test fold index (0 <= fold_num < maxFold)\n",
    "        lmdb_keys: LMDB key list\n",
    "        maxFold: total number of folds (k)\n",
    "        split_by_sub: True → subject-wise k-fold, False → key-wise k-fold\n",
    "\n",
    "    Returns:\n",
    "        train_key_list, test_key_list\n",
    "    \"\"\"\n",
    "    if maxFold < 2:\n",
    "        raise ValueError(\"maxFold must be >= 2.\")\n",
    "    if fold_num < 0 or fold_num >= maxFold:\n",
    "        raise ValueError(f\"fold_num must be in [0, {maxFold-1}]\")\n",
    "\n",
    "    keys = list(lmdb_keys)\n",
    "\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    if split_by_sub:\n",
    "        # -------- subject-wise k-fold --------\n",
    "        sub_to_keys = defaultdict(list)\n",
    "        invalid = []\n",
    "\n",
    "        for k in keys:\n",
    "            try:\n",
    "                sid = _extract_sub_id(k)\n",
    "                sub_to_keys[sid].append(k)\n",
    "            except ValueError:\n",
    "                invalid.append(_decode_key(k))\n",
    "\n",
    "        if invalid:\n",
    "            ex = \"\\n\".join(invalid[:10])\n",
    "            raise ValueError(\n",
    "                f\"Found {len(invalid)} invalid keys. Examples:\\n{ex}\"\n",
    "            )\n",
    "\n",
    "        subjects = np.array(list(sub_to_keys.keys()), dtype=object)\n",
    "        rng.shuffle(subjects)\n",
    "\n",
    "        subj_folds = np.array_split(subjects, maxFold)\n",
    "        test_subjects = set(subj_folds[fold_num].tolist())\n",
    "\n",
    "        train_keys, test_keys = [], []\n",
    "        for sid, ks in sub_to_keys.items():\n",
    "            (test_keys if sid in test_subjects else train_keys).extend(ks)\n",
    "\n",
    "        return train_keys, test_keys\n",
    "\n",
    "    else:\n",
    "        # -------- key-wise k-fold --------\n",
    "        idx = np.arange(len(keys))\n",
    "        rng.shuffle(idx)\n",
    "\n",
    "        folds = np.array_split(idx, maxFold)\n",
    "        test_idx = set(folds[fold_num].tolist())\n",
    "\n",
    "        train_keys = [keys[i] for i in idx if i not in test_idx]\n",
    "        test_keys  = [keys[i] for i in idx if i in test_idx]\n",
    "\n",
    "        return train_keys, test_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829dc72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# /pscratch/sd/a/ahhyun/EcoGFound/DATA/scaling_data_V2_Sep_2025/striped_EEG_lmdb\n",
    "# 아현썜 pscratch의 데이터 경로 당장은 그냥 써도 되지만 추후 내 pscratch나 m4727 등으로 옮겨서 사용할 것\n",
    "\n",
    "class TUEG_for_SOLID_from_lmdb(Dataset):\n",
    "    def __init__(\n",
    "            self,\n",
    "            lmdb_dir: str,\n",
    "            maxfold: int,\n",
    "            targetfold: int,\n",
    "            seed: int,\n",
    "            train: bool,\n",
    "            split_by_sub: bool,\n",
    "    ):\n",
    "        random_seed(seed)\n",
    "        self.seed = seed\n",
    "        self.lmdb_dir = lmdb_dir\n",
    "        self.db = lmdb.open(lmdb_dir, readonly=True, lock=False, readahead=True, meminit=False)\n",
    "        with self.db.begin(write=False) as txn:\n",
    "            self.lmdb_keys = pickle.loads(txn.get('__keys__'.encode()))\n",
    "\n",
    "        self.train = train\n",
    "        self.split_by_sub = split_by_sub\n",
    "\n",
    "        self.maxfold = maxfold\n",
    "        self.targetfold = targetfold\n",
    "        self.data, self.target = self.make_data_and_target_by_fold(self.targetfold, self.lmdb_keys, \n",
    "                                                                   self.maxfold, self.split_by_sub, self.seed)\n",
    "\n",
    "    def make_data_and_target_by_fold(self, fold, lmdb_keys, maxfold, split_by_sub, seed):\n",
    "        self.record = []\n",
    "\n",
    "        train_data = {'input':[], 'target':[]}\n",
    "        test_data = {'input':[], 'target':[]}\n",
    "\n",
    "        train_data_keys_in_lmdb, test_data_keys_in_lmdb = train_test_split_by_fold_num(fold, lmdb_keys, maxfold, split_by_sub, seed)\n",
    "\n",
    "\n",
    "        if self.train:\n",
    "            for train_data_key in train_data_keys_in_lmdb:\n",
    "\n",
    "                # TODO : get proper seg_in and seg_out by input idx\n",
    "                seg_in, seg_out = self.segmentation_from_idx(train_data_key, self.db)\n",
    "\n",
    "                train_data['input'] += seg_in\n",
    "                train_data['target'] += seg_out\n",
    "\n",
    "                data = train_data['input']\n",
    "                target = train_data['target']\n",
    "        \n",
    "        else:\n",
    "            for test_data_key in test_data_keys_in_lmdb:\n",
    "\n",
    "                seg_in, seg_out = self.segmentation_from_idx(test_data_key, self.db)\n",
    "\n",
    "                test_data['input'] += seg_in\n",
    "                test_data['target'] += seg_out\n",
    "\n",
    "                data = test_data['input']\n",
    "                target = test_data['target']\n",
    "\n",
    "\n",
    "        return data, target\n",
    "\n",
    "    def lmdb_get(self, env, key):\n",
    "        if isinstance(key, str):\n",
    "            key = key.encode(\"utf-8\")\n",
    "        with env.begin(write=False) as txn:\n",
    "            v = txn.get(key)\n",
    "        if v is None:\n",
    "            raise KeyError(f\"Key not found: {key}\")\n",
    "        return pickle.loads(v)\n",
    "\n",
    "    def segmentation_from_idx(self, key, lmdb_db):\n",
    "        sample_for_key = self.lmdb_get(lmdb_db, key)\n",
    "\n",
    "        channel_name = sample_for_key['data_info']['channel_name'] # (C,) list\n",
    "        eeg_data = sample_for_key['sample'] # (C, T, Fs)\n",
    "        eeg_data_ = rearrange(eeg_data, 'c t f -> c (t f)')\n",
    "        \n",
    "        seg_in = None\n",
    "        seg_out = None\n",
    "        return seg_in, seg_out\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        input_, target_ = self.data[idx], self.target[idx]\n",
    "        # TODO : 이거 meta 줄 떄 time은 time이고 spatial을 아예 grid에 맞게 주는게 좋을 듯 / Grid 여기서 받게 하자\n",
    "        i = None\n",
    "        o = None\n",
    "        im = None\n",
    "        om = None\n",
    "        return i, o, im, om"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe198315",
   "metadata": {},
   "outputs": [],
   "source": [
    "TORCHEEG_2DGRID = [\n",
    "    ['-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-'],\n",
    "    ['-', '-', '-', '-', 'FP1', 'FPZ', 'FP2', '-', '-', '-', '-'],\n",
    "    ['-', '-', 'AF7', '-', 'AF3', 'AFZ', 'AF4', '-', 'AF8', '-', '-'],\n",
    "    ['F9', 'F7', 'F5', 'F3', 'F1', 'FZ', 'F2', 'F4', 'F6', 'F8', 'F10'],\n",
    "    ['FT9', 'FT7', 'FC5', 'FC3', 'FC1', 'FCZ', 'FC2', 'FC4', 'FC6', 'FT8', 'FT10'], \n",
    "    ['T9', 'T7', 'C5', 'C3', 'C1', 'CZ', 'C2', 'C4', 'C6', 'T8', 'T10'],\n",
    "    ['TP9', 'TP7', 'CP5', 'CP3', 'CP1', 'CPZ', 'CP2', 'CP4', 'CP6', 'TP8', 'TP10'], \n",
    "    ['P9', 'P7', 'P5', 'P3', 'P1', 'PZ', 'P2', 'P4', 'P6', 'P8', 'P10'],\n",
    "    ['-', '-', 'PO7', 'PO5', 'PO3', 'POZ', 'PO4', 'PO6', 'PO8', '-', '-'],\n",
    "    ['-', '-', '-', 'CB1', 'O1', 'OZ', 'O2', 'CB2', '-', '-', '-'],\n",
    "    ['-', '-', '-', '-', '-', 'IZ', '-', '-', '-', '-', '-']\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fb5d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEGToGrid(Dataset):\n",
    "    def __init__(self, base_dataset,):\n",
    "        self.base_dataset = base_dataset\n",
    "        self.mean = float(self.base_dataset.mean)\n",
    "        self.std = float(self.base_dataset.std)\n",
    "\n",
    "    def TorchEEG_Grid(self, channel_list, grid_templete=TORCHEEG_2DGRID, H=11, W=11):\n",
    "        \"\"\"\n",
    "        2D Grid based on TorchEEG 2D Grid\n",
    "        input 10-10 coord channel name index \n",
    "        output is grid of channel input\n",
    "        \"\"\"\n",
    "        grid = torch.zeros(H, W, dtype=torch.float32)\n",
    "        mask = torch.zeros(H, W, dtype=torch.float32)\n",
    "        return grid, mask\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.base_dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        i,o,im,om = self.base[idx]\n",
    "\n",
    "        target_grid = None\n",
    "        target_mask = None\n",
    "        cond = None\n",
    "\n",
    "        return target_grid, target_mask, cond, self.mean, self.std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200d6917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set some future argparse\n",
    "\n",
    "# TUEG_1.0 path in lucy's pscratch\n",
    "# /pscratch/sd/a/ahhyun/EcoGFound/DATA/scaling_data_V2_Sep_2025/striped_EEG_lmdb/TUEG_1.0/1.0_TUEG/all_resample-500_highpass-0.3_lowpass-None.lmdb\n",
    "LMDB_DIR = \"/pscratch/sd/a/ahhyun/EcoGFound/DATA/scaling_data_V2_Sep_2025/striped_EEG_lmdb/TUEG_1.0/1.0_TUEG/all_resample-500_highpass-0.3_lowpass-None.lmdb\"\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c798205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Train and Test dataset and dataloader\n",
    "\n",
    "train_eeg = TUEG_for_SOLID_from_lmdb(lmdb_dir=LMDB_DIR,\n",
    "                         maxFolds=5,\n",
    "                         seed=41,\n",
    "                         train=True,)\n",
    "test_eeg = TUEG_for_SOLID_from_lmdb(lmdb_dir=LMDB_DIR,\n",
    "                         maxFolds=5,\n",
    "                         seed=41,\n",
    "                         train=False,\n",
    "                         )\n",
    "\n",
    "train_set = EEGToGrid(train_eeg)\n",
    "test_set = EEGToGrid(test_eeg)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True, drop_last=True, num_worker=2, pin_memory=True)\n",
    "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False, drop_last=False, num_worker=2, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2c0642",
   "metadata": {},
   "outputs": [],
   "source": [
    "LMDB_DIR = \"/pscratch/sd/t/tylee/Dataset/1109_Physio_500Hz\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de179f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEG_from_lmdb(Dataset):\n",
    "    def __init__(self, data_dir, transform, return_info):\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "        self.return_info = return_info\n",
    "\n",
    "    def lmdb_to_data(self, idx):\n",
    "        self.db = lmdb.open(self.data_dir, readonly=True, lock=False, readahead=True, meminit=False)\n",
    "        key = self.keys[idx]\n",
    "        with self.db.begin(write=False) as txn:\n",
    "            pair = pickle.loads(txn.get(key.encode()))\n",
    "        data = pair['sample']\n",
    "        label = pair['label']\n",
    "        data_info = pair.get('data_info', {})\n",
    "        \n",
    "        data = to_tensor(data)\n",
    "        if self.transform is not None:\n",
    "            data = self.transform(data)\n",
    "        if self.return_info:\n",
    "            return data/100, label, data_info\n",
    "        else:\n",
    "            return data/100, label\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_, target_ = self.data[idx], self.target[idx]\n",
    "        in_ = torch.from_numpy(input_.astype(np.float32))\n",
    "        out_ = torch.from_numpy(target_.astype(np.float32))\n",
    "        # print(in_)\n",
    "        \n",
    "        # Normalize pm2.5 values\n",
    "        in_[..., 0] = self.normalize_z(in_[..., 0])\n",
    "        out_[..., 0] = self.normalize_z(out_[..., 0])\n",
    "        \n",
    "        in_[..., 1] = in_[..., 1] / 1440\n",
    "        out_[..., 1] = out_[..., 1] / 1440\n",
    "        \n",
    "        timegap = out_[..., 1:2][0] # get the gap between t+1 and t (ignoring t-1, and t-2)\n",
    "        \n",
    "        in_ = torch.cat([in_[..., 0:1], in_[..., 2:], in_[..., 1:2], ], dim=-1)\n",
    "        out_ = torch.cat([out_[..., 0:1], out_[..., 2:], out_[..., 1:2], ], dim=-1)\n",
    "        \n",
    "        in_[..., 1] = self.normalize(in_[..., 1], self.latmin, self.latmax)\n",
    "        out_[..., 1] = self.normalize(out_[..., 1], self.latmin, self.latmax)\n",
    "        in_[..., 2] = self.normalize(in_[..., 2], self.longmin, self.longmax)\n",
    "        out_[..., 2] = self.normalize(out_[..., 2], self.longmin, self.longmax)\n",
    "        \n",
    "        i = in_[..., 0:1]\n",
    "        im = in_[..., 1:]\n",
    "        o = out_[..., 0:1]\n",
    "        om = out_[..., 1:]\n",
    "        \n",
    "        return i, o, im, om"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c5fd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing torch.dataset\n",
    "\n",
    "def xform_day(day):\n",
    "    arr = [0, 30, 61]\n",
    "    w = 0 if day <= 30 else 1 if day <= 61 else 2\n",
    "    mon = ['2020-11-', '2020-12-', '2021-01-'][w]\n",
    "    date = mon + '{:02d}'.format(day - arr[w])\n",
    "    return date\n",
    "\n",
    "\n",
    "def get_suffixes(mode):\n",
    "    suffixes = []\n",
    "    if 'C' in mode or 'A' in mode:\n",
    "        suffixes.append('train')\n",
    "    if 'D' in mode or 'B' in mode:\n",
    "        suffixes.append('test')\n",
    "    return suffixes\n",
    "\n",
    "def rename_cols(data):\n",
    "    data.rename(\n",
    "        columns={'dateTime': 'time', 'lat': 'latitude', 'long': 'longitude', 'pm2_5': 'PM25_Concentration',\n",
    "                 'pm10': 'PM10_Concentration'}, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "def torch1dgrid(num, bot=0, top=1):\n",
    "    arr = torch.linspace(bot, top, steps=num)\n",
    "    mesh = torch.stack([arr], dim=1)\n",
    "    return mesh.squeeze(-1)\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from einops import rearrange        \n",
    "class Delhi(Dataset):\n",
    "    def __init__(\n",
    "        self, mode_t, mode_p, canada, train_days, \n",
    "        maxFolds = 5, target_fold = 0, temporal_scaling=1, spatiotemporal=1, data_dir='/pscratch/sd/d/dpark1/AirDelhi/delhi/processed', \n",
    "        seed=10, nTrainStartDay = 15, nTestStartDay = 75, nTotalDays = 91, train=True):\n",
    "        \n",
    "        self.mode_t = mode_t\n",
    "        self.mode_p = mode_p\n",
    "        self.train_days = train_days\n",
    "        self.train = train\n",
    "        self.maxFolds = maxFolds\n",
    "        self.target_fold = target_fold\n",
    "        self.temporal_scaling = temporal_scaling\n",
    "        self.spatiotemporal = spatiotemporal\n",
    "        self.data_dir = data_dir\n",
    "        self.nTestStartDay = nTestStartDay\n",
    "        self.nTrainStartDay = nTrainStartDay\n",
    "        self.nTotalDays = nTotalDays\n",
    "        \n",
    "        np.random.seed(seed)        \n",
    "        \n",
    "        self.train_suffix = get_suffixes(mode_t)\n",
    "                \n",
    "        if spatiotemporal < 0 and mode_t == 'AB' and mode_p == 'CD':\n",
    "            # Forecasting, single fold is enough\n",
    "            maxFolds = 1\n",
    "    \n",
    "        self.folds = [i for i in range(maxFolds)]\n",
    "        \n",
    "        self.data, self.target = self.proc_custom(target_fold)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def get_normalize_params(self, target):\n",
    "        all_signal = []\n",
    "        for a in target:\n",
    "            all_signal += list(a[..., 0])\n",
    "        self.mean, self.std = np.array(all_signal).mean(), np.array(all_signal).std()\n",
    "    \n",
    "    def get_spatial_norm_parameters(self, arr_of_days):\n",
    "        \"\"\"\"minmax normalization\"\"\"\n",
    "        latmin = 10e10\n",
    "        latmax = -10e10\n",
    "        longmin = 10e10\n",
    "        longmax = -10e10\n",
    "        for arr in arr_of_days:\n",
    "            minned = arr.min(0)\n",
    "            # print(minned[0])\n",
    "            if minned[2] < latmin:\n",
    "                latmin = minned[2]\n",
    "            if minned[3] < longmin:\n",
    "                longmin = minned[3]\n",
    "                \n",
    "            maxed = arr.max(0)\n",
    "            # print(maxed)\n",
    "            if maxed[2] > latmax:\n",
    "                latmax = maxed[2]\n",
    "            if maxed[3] > longmax:\n",
    "                longmax = maxed[3]\n",
    "        self.latmin, self.latmax, self.longmin, self.longmax =latmin, latmax, longmin, longmax\n",
    "    \n",
    "    def make_data_by_time(self, arr_of_days, t_in = 9, reverse=False, day = 0):\n",
    "        seg_by_time = []\n",
    "        uniq_times = np.unique(arr_of_days[..., 1])\n",
    "        \n",
    "        for t in uniq_times:\n",
    "            idx_ = arr_of_days[..., 1] == t\n",
    "            seg_by_time.append(arr_of_days[idx_])\n",
    "        \n",
    "        in_ = []\n",
    "        out_ = []\n",
    "        for i in range(len(seg_by_time) - t_in):\n",
    "            temp_in = []\n",
    "            for t_ in range(t_in):\n",
    "                temp_in.append(seg_by_time[i + t_])\n",
    "            \n",
    "            # normalize time to relative scale by the last one of the encoder\n",
    "            in_cand = np.copy(np.concatenate(temp_in, axis=0))\n",
    "            out_cand = np.copy(seg_by_time[i + t_in])\n",
    "            \n",
    "            last_enc_t = in_cand[..., 1][-1]\n",
    "            in_cand[..., 1] -= last_enc_t\n",
    "            out_cand[..., 1] -= last_enc_t\n",
    "            in_.append(in_cand)\n",
    "            out_.append(out_cand)\n",
    "            self.day_record.append(day)\n",
    "            \n",
    "            # reverse it\n",
    "            if reverse:\n",
    "                out_cand = np.copy(seg_by_time[i])\n",
    "                temp_in = temp_in[1:]\n",
    "                temp_in.append(seg_by_time[i + t_in])\n",
    "                in_cand = np.copy(np.concatenate(temp_in, axis=0))\n",
    "                last_enc_t = in_cand[..., 1][0]\n",
    "                in_cand[..., 1] -= last_enc_t\n",
    "                out_cand[..., 1] -= last_enc_t\n",
    "                \n",
    "                in_.append(in_cand)\n",
    "                out_.append(out_cand)\n",
    "                self.day_record.append(day)\n",
    "        \n",
    "        \n",
    "        return in_, out_\n",
    "    \n",
    "    def proc_custom(self, fold):\n",
    "        \n",
    "        self.day_record = []\n",
    "        \n",
    "        train_data = {'input':[], 'target':[]}\n",
    "        test_data = {'input':[], 'target':[]}\n",
    "        \n",
    "        for day in range(self.nTrainStartDay, self.nTestStartDay):\n",
    "            date = []\n",
    "            for i in range(self.train_days,-1,-1):\n",
    "                date.append(xform_day(day-i))\n",
    "\n",
    "            train_input,train_output,test_input,test_output = self.process_np(fold, date)\n",
    "            train_in = np.concatenate([train_output[..., np.newaxis], train_input], axis=1) # 1 days\n",
    "            train_out = np.concatenate([test_output[..., np.newaxis], test_input], axis=1) # 1 day\n",
    "            \n",
    "            seg_in, seg_out = self.make_data_by_time(train_in, day = day)\n",
    "            \n",
    "            train_data['input'] += seg_in\n",
    "            train_data['target'] += seg_out            \n",
    "        \n",
    "        \n",
    "        \n",
    "        seg_in, seg_out = self.make_data_by_time(train_out)\n",
    "        train_data['input'] += seg_in\n",
    "        train_data['target'] += seg_out\n",
    "            \n",
    "        \n",
    "        for day in range(self.nTestStartDay, self.nTotalDays+1):\n",
    "            date = []\n",
    "            for i in range(self.train_days,-1,-1):\n",
    "                date.append(xform_day(day-i))\n",
    "\n",
    "            train_input,train_output,test_input,test_output = self.process_np(fold, date)\n",
    "            test_in = np.concatenate([train_output[..., np.newaxis], train_input], axis=1) # 1 days\n",
    "            test_out = np.concatenate([test_output[..., np.newaxis], test_input], axis=1) # 1 day\n",
    "\n",
    "            seg_in, seg_out = self.make_data_by_time(test_in, reverse = False)\n",
    "            \n",
    "            test_data['input'] += seg_in\n",
    "            test_data['target'] += seg_out            \n",
    "            \n",
    "        seg_in, seg_out = self.make_data_by_time(test_out, reverse = False)\n",
    "        test_data['input'] += seg_in\n",
    "        test_data['target'] += seg_out\n",
    "\n",
    "        self.get_normalize_params(train_data['target']) \n",
    "        self.get_spatial_norm_parameters(train_data['target'])\n",
    "            \n",
    "        if self.train:\n",
    "            data = train_data['input']\n",
    "            target = train_data['target']\n",
    "            print(len(data), len(target))\n",
    "            \n",
    "        else:\n",
    "            data = test_data['input']\n",
    "            target = test_data['target']\n",
    "            print(len(data), len(target))\n",
    "\n",
    "        return data, target        \n",
    "    \n",
    "    \n",
    "\n",
    "    def process_np(self, fold, date):\n",
    "        tmStart = datetime.datetime.now()\n",
    "        train_input,train_output,test_input,test_output = self.return_data_time(fold=fold, data=date, with_scaling=True)\n",
    "        return train_input,train_output,test_input,test_output\n",
    "    \n",
    "    def return_data_time(self, fold, data, with_scaling):\n",
    "        train_input = None\n",
    "        if 'A' in self.mode_t or 'B' in self.mode_t:\n",
    "            for idx,dt in enumerate(data[:-1]):\n",
    "                for suffix in self.train_suffix:\n",
    "                    input = pd.read_csv(self.data_dir+'/'+dt+'_f'+str(fold)+'_'+suffix+'.csv')\n",
    "                    # if self.temporal_scaling:\n",
    "                    #     input.dateTime += idx * 24 * 60\n",
    "                    train_input = pd.concat((train_input, input))\n",
    "                    \n",
    "        if 'C' in self.mode_t:\n",
    "            input = pd.read_csv(self.data_dir + '/' + data[-1] + '_f' + str(fold) + '_train.csv')\n",
    "            # if self.temporal_scaling:\n",
    "            #     input.dateTime += (len(data)-1) * 24 * 60\n",
    "            train_input = pd.concat((train_input, input))\n",
    "\n",
    "        test_input = pd.read_csv(self.data_dir+'/'+data[-1]+'_f'+str(fold)+'_test.csv')\n",
    "        \n",
    "        if 'C' in self.mode_p:\n",
    "            input = pd.read_csv(self.data_dir + '/' + data[-1] + '_f' + str(fold) + '_train.csv')\n",
    "            test_input = pd.concat((input, test_input))\n",
    "            \n",
    "        # if self.temporal_scaling:\n",
    "        #     test_input.dateTime += (len(data)-1) * 24 * 60\n",
    "\n",
    "        return self.return_data_0(train_input, test_input, with_scaling)\n",
    "\n",
    "    \n",
    "    def return_data_0(self, train_input, test_input, with_scaling):\n",
    "        train_output = np.array(train_input['pm2_5'])\n",
    "        train_input = train_input[['dateTime','lat','long']]\n",
    "        test_output = np.array(test_input['pm2_5'])\n",
    "        test_input = test_input[['dateTime','lat','long']]\n",
    "\n",
    "        # if with_scaling:\n",
    "        #     scaler = MinMaxScaler().fit(train_input)\n",
    "        #     if self.temporal_scaling:\n",
    "        #         data = scaler.transform(pd.concat((train_input, test_input)))\n",
    "        #         test_input = data[len(train_input):]\n",
    "        #         train_input = data[:len(train_input)]\n",
    "        #     else:\n",
    "        #         train_input = scaler.transform(train_input)\n",
    "        #         test_input = scaler.transform(test_input)\n",
    "        return train_input,train_output,test_input,test_output\n",
    "\n",
    "    def set_target_fold(self, fold=0):\n",
    "        self.fold = fold\n",
    "        print('target fold set to {}'.format(self.fold))\n",
    "        \n",
    "    def normalize_z(self, arr):\n",
    "        return (arr - self.mean) / self.std\n",
    "    \n",
    "    def normalize(self, data, min_, max_):\n",
    "        return (data - min_) / (max_ - min_)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_, target_ = self.data[idx], self.target[idx]\n",
    "        in_ = torch.from_numpy(input_.astype(np.float32))\n",
    "        out_ = torch.from_numpy(target_.astype(np.float32))\n",
    "        # print(in_)\n",
    "        \n",
    "        # Normalize pm2.5 values\n",
    "        in_[..., 0] = self.normalize_z(in_[..., 0])\n",
    "        out_[..., 0] = self.normalize_z(out_[..., 0])\n",
    "        \n",
    "        in_[..., 1] = in_[..., 1] / 1440\n",
    "        out_[..., 1] = out_[..., 1] / 1440\n",
    "        \n",
    "        timegap = out_[..., 1:2][0] # get the gap between t+1 and t (ignoring t-1, and t-2)\n",
    "        \n",
    "        in_ = torch.cat([in_[..., 0:1], in_[..., 2:], in_[..., 1:2], ], dim=-1)\n",
    "        out_ = torch.cat([out_[..., 0:1], out_[..., 2:], out_[..., 1:2], ], dim=-1)\n",
    "        \n",
    "        in_[..., 1] = self.normalize(in_[..., 1], self.latmin, self.latmax)\n",
    "        out_[..., 1] = self.normalize(out_[..., 1], self.latmin, self.latmax)\n",
    "        in_[..., 2] = self.normalize(in_[..., 2], self.longmin, self.longmax)\n",
    "        out_[..., 2] = self.normalize(out_[..., 2], self.longmin, self.longmax)\n",
    "        \n",
    "        i = in_[..., 0:1]\n",
    "        im = in_[..., 1:]\n",
    "        o = out_[..., 0:1]\n",
    "        om = out_[..., 1:]\n",
    "        \n",
    "        return i, o, im, om"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fingerflex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
